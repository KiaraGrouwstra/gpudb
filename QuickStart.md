

# GPUDB Start Guide #

## Summary ##
This is a translator that translates a given SQL query into CUDA codes (executed on NVIDIA GPUs) or OpenCL codes (executed on devices that support OpenCL). You need to:

  1. Define the schema and generate the raw text-based data.
  1. Generate the data loader that transform the raw data into our supported data format.
  1. (Optional) Compress the data to obtain better performance.
  1. Translate the SQL query and execute the query.

The remaining contents in this guide will explain each step in details.
All the paths used in the following content are relative to the top directory of the source code.

## System Requirement ##

  * A Linux system with Python installed.
  * CUDA toolkit (4.0 or later), AMD APP SDK or Intel OpenCL SDK installed.
  * NVIDIA GPUs, AMD GPUs or CPUs that support OpenCL.

## Source Code ##
You can check out the source codes [here](http://code.google.com/p/gpudb/source/checkout).


## Write Schema File ##

Please refer to wiki page Schema to learn how to write schema file. An example schema file is located in test/ssb\_test/ssb.schema.

## Data Generation ##

Please refer to StorageFormat to get a general idea of the format of the data. You must ensure that the data are stored in the correct format before running the queries. We provide a tool that can transform the plain text data (columns must be separated by '|') into our supported storage format.

### Generate Data Loader ###

Execute
  * ./translate.py your\_schema\_file

to generate a C file that can be used to transform the raw text data into our supported data format. The C file is generated in src/utility/load.c.

For SSBM queries, execute the following command:

  * ./translate.py test/ssb\_test/ssb.schema


The following shows how to transform the original SSBM data into our supported storage format.


### SSBM Plain Data Generation ###
The dbgen to generate original SSBM data is located
in test/dbgen directory. You need to generate the plain
text data. The following steps show how to generate the data with a scale factor of 1.

  * Enter test/dbgen directory, and execute "make"
  * Execute "./dbgen -vfF -T a -s 1" to generate data with scale factor 1


### Load SSBM Data ###
The original SSBM data cannot be directly used by our engine.
The data need to be transformed into column-stored data. The following steps show how to transform the original SSBM data.

  * Enter src/utility directory and execute "make loader", which will generate "gpuDBLoader" to transform the original data.

  * To load data for the previously generated SSBM data, execute the following command: ./gpuDBLoader --lineorder ../../test/dbgen/lineorder.tbl --ddate ../../test/dbgen/date.tbl --customer ../../test/dbgen/customer.tbl --supplier ../../test/dbgen/supplier.tbl --part ../../test/dbgen/part.tbl

Generally speaking, gpuDBLoader accepts arguments in the format "--table\_name table\_file". Each table column is stored in a disk file whose name is composed of the table name in upper case followed by the index of the column in the table. Users can specify where the data will be generated by using "--datadir dir". The data will be generated in the current directory by default.

## Data Compression ##

Data can be compressed to get a better query performance. You can skip
to the next section if you don't need to compress the data.

#### Run Length Encoding ####

  * Sort. To get a better compression ratio, the column that is compressed using RLE scheme should be sorted first. To sort the column, users can either use the "sort" command to sort the plain text SSBM data on particular columns before transforming the data into column-stored formant, or they can enter src/utility directory and execute "make sort" to generate a file named "columnSort" that will sort a table on a particular integer column.

> Usage: ./columnSort inputPrefix outputPrefix index. inputPrefix is the name of the table to be sorted, outputPrefix is the name of the generated sorted table and index specifies the sorted column.

  * Compression. Enter src/utility and execute "make rle" to generate the file "rleCompression" to compress the column using RLE scheme.

> Usage: ./rleCompression inputColumn outputColumn. inputColumn specifies the name of the sorted column to be compressed using RLE, and outputColumn is the name of the generated compressed column.

#### Dictionary Encoding ####

> Enter src/utility and execute "make dict" to generate the file "dictCompression" to compress the column using dictionary encoding scheme.

> Usage: ./dictCompression inputColumn outputColumn. inputColumn specifies the name of the column to be compressed using dictionary encoding scheme, and outputColumn is the name of the generated compressed column.

## Code Generation ##

Usage: **./translate.py query-file.sql schema-file.schema**
  * query-file.sql should contain the SQL query you want to translate. Currently the suppported SQL queries are listed in test/ssb\_test directory
  * schema-file.schema is the schema file that describes the data. Currently the schema file for star schema benchmark is in test/ssb\_test/ssb.schema.

After executing the aforementioned command, enter src/cuda or src/opencl directory and execute "Make gpudb", which will generate an executable file
named "GPUDATABASE". Executing "./GPUDATABASE --datadir dir" will run the query on GPU. The "datadir" argument specifies where the data are stored.